{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_baseline.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight,gain=nn.init.calculate_gain('relu'))\n",
      "[1,     3] loss: 6.485\n",
      "acc:0.1302 \n",
      "err:0.8698 \n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Validation RESULT]\n",
      "acc:0.1229 \n",
      "err:0.8771\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.12      1.00      0.22       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.12      1050\n",
      "   macro avg       0.02      0.12      0.03      1050\n",
      "weighted avg       0.02      0.12      0.03      1050\n",
      "\n",
      "[2,     3] loss: 12.885\n",
      "acc:0.1524 \n",
      "err:0.8476 \n",
      "[Validation RESULT]\n",
      "acc:0.0857 \n",
      "err:0.9143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       129\n",
      "         1.0       0.09      1.00      0.16        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.09      1050\n",
      "   macro avg       0.01      0.12      0.02      1050\n",
      "weighted avg       0.01      0.09      0.01      1050\n",
      "\n",
      "[3,     3] loss: 9.182\n",
      "acc:0.0984 \n",
      "err:0.9016 \n",
      "[Validation RESULT]\n",
      "acc:0.1362 \n",
      "err:0.8638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.14      1.00      0.24       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.14      1050\n",
      "   macro avg       0.02      0.12      0.03      1050\n",
      "weighted avg       0.02      0.14      0.03      1050\n",
      "\n",
      "[4,     3] loss: 1.960\n",
      "acc:0.1302 \n",
      "err:0.8698 \n",
      "[Validation RESULT]\n",
      "acc:0.1390 \n",
      "err:0.8610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       0.14      1.00      0.24       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.14      1050\n",
      "   macro avg       0.02      0.12      0.03      1050\n",
      "weighted avg       0.02      0.14      0.03      1050\n",
      "\n",
      "[5,     3] loss: 2.176\n",
      "acc:0.1556 \n",
      "err:0.8444 \n",
      "[Validation RESULT]\n",
      "acc:0.1771 \n",
      "err:0.8229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.17      0.28      0.21       152\n",
      "         3.0       0.18      1.00      0.30       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.18      1050\n",
      "   macro avg       0.04      0.16      0.06      1050\n",
      "weighted avg       0.05      0.18      0.07      1050\n",
      "\n",
      "[6,     3] loss: 1.272\n",
      "acc:0.1683 \n",
      "err:0.8317 \n",
      "[Validation RESULT]\n",
      "acc:0.1781 \n",
      "err:0.8219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.23      0.38       129\n",
      "         1.0       0.09      1.00      0.17        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       1.00      0.05      0.10       146\n",
      "         5.0       0.94      0.40      0.56       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.18      1050\n",
      "   macro avg       0.38      0.21      0.15      1050\n",
      "weighted avg       0.40      0.18      0.15      1050\n",
      "\n",
      "[7,     3] loss: 0.747\n",
      "acc:0.2508 \n",
      "err:0.7492 \n",
      "[Validation RESULT]\n",
      "acc:0.1629 \n",
      "err:0.8371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      1.00      0.29       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       0.89      0.11      0.20       146\n",
      "         5.0       0.00      0.00      0.00       148\n",
      "         6.0       0.10      0.21      0.14       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.16      1050\n",
      "   macro avg       0.14      0.17      0.08      1050\n",
      "weighted avg       0.16      0.16      0.08      1050\n",
      "\n",
      "[8,     3] loss: 0.531\n",
      "acc:0.2095 \n",
      "err:0.7905 \n",
      "[Validation RESULT]\n",
      "acc:0.2324 \n",
      "err:0.7676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.08      0.14       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.17      0.84      0.28       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       0.34      0.77      0.47       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.23      1050\n",
      "   macro avg       0.19      0.21      0.11      1050\n",
      "weighted avg       0.19      0.23      0.12      1050\n",
      "\n",
      "[9,     3] loss: 0.421\n",
      "acc:0.3079 \n",
      "err:0.6921 \n",
      "[Validation RESULT]\n",
      "acc:0.3057 \n",
      "err:0.6943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.66      0.76       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       1.00      0.29      0.45       143\n",
      "         4.0       0.84      0.33      0.47       146\n",
      "         5.0       0.17      0.99      0.29       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.31      1050\n",
      "   macro avg       0.36      0.28      0.25      1050\n",
      "weighted avg       0.39      0.31      0.26      1050\n",
      "\n",
      "[10,     3] loss: 0.489\n",
      "acc:0.3143 \n",
      "err:0.6857 \n",
      "[Validation RESULT]\n",
      "acc:0.2314 \n",
      "err:0.7686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.66      0.75       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.00      0.00      0.00       143\n",
      "         4.0       0.00      0.00      0.00       146\n",
      "         5.0       1.00      0.14      0.25       148\n",
      "         6.0       0.13      0.99      0.24       122\n",
      "         7.0       0.64      0.13      0.22       120\n",
      "\n",
      "    accuracy                           0.23      1050\n",
      "   macro avg       0.33      0.24      0.18      1050\n",
      "weighted avg       0.34      0.23      0.18      1050\n",
      "\n",
      "[11,     3] loss: 0.296\n",
      "acc:0.2698 \n",
      "err:0.7302 \n",
      "[Validation RESULT]\n",
      "acc:0.3467 \n",
      "err:0.6533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.66      0.66       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.20      0.95      0.33       143\n",
      "         4.0       0.90      0.36      0.51       146\n",
      "         5.0       0.46      0.61      0.53       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.35      1050\n",
      "   macro avg       0.28      0.32      0.25      1050\n",
      "weighted avg       0.30      0.35      0.27      1050\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,     3] loss: 0.281\n",
      "acc:0.3778 \n",
      "err:0.6222 \n",
      "[Validation RESULT]\n",
      "acc:0.3971 \n",
      "err:0.6029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.47      0.64       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.18      0.70      0.28       152\n",
      "         3.0       0.83      0.35      0.49       143\n",
      "         4.0       0.59      0.43      0.50       146\n",
      "         5.0       1.00      0.17      0.29       148\n",
      "         6.0       0.46      0.57      0.51       122\n",
      "         7.0       0.93      0.35      0.51       120\n",
      "\n",
      "    accuracy                           0.40      1050\n",
      "   macro avg       0.62      0.38      0.40      1050\n",
      "weighted avg       0.65      0.40      0.41      1050\n",
      "\n",
      "[13,     3] loss: 0.192\n",
      "acc:0.4889 \n",
      "err:0.5111 \n",
      "[Validation RESULT]\n",
      "acc:0.4857 \n",
      "err:0.5143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       0.57      0.59      0.58       143\n",
      "         4.0       0.56      0.43      0.49       146\n",
      "         5.0       0.54      0.69      0.61       148\n",
      "         6.0       0.27      0.89      0.42       122\n",
      "         7.0       0.56      0.57      0.56       120\n",
      "\n",
      "    accuracy                           0.49      1050\n",
      "   macro avg       0.44      0.48      0.43      1050\n",
      "weighted avg       0.45      0.49      0.44      1050\n",
      "\n",
      "[14,     3] loss: 0.142\n",
      "acc:0.4635 \n",
      "err:0.5365 \n",
      "[Validation RESULT]\n",
      "acc:0.3848 \n",
      "err:0.6152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.97      0.41       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.28      0.68      0.40       152\n",
      "         3.0       1.00      0.21      0.35       143\n",
      "         4.0       0.93      0.36      0.51       146\n",
      "         5.0       0.92      0.53      0.67       148\n",
      "         6.0       0.69      0.09      0.16       122\n",
      "         7.0       0.40      0.03      0.06       120\n",
      "\n",
      "    accuracy                           0.38      1050\n",
      "   macro avg       0.56      0.36      0.32      1050\n",
      "weighted avg       0.59      0.38      0.35      1050\n",
      "\n",
      "[15,     3] loss: 0.120\n",
      "acc:0.4317 \n",
      "err:0.5683 \n",
      "[Validation RESULT]\n",
      "acc:0.4505 \n",
      "err:0.5495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.65      0.79       129\n",
      "         1.0       0.30      0.68      0.42        90\n",
      "         2.0       0.00      0.00      0.00       152\n",
      "         3.0       1.00      0.31      0.47       143\n",
      "         4.0       0.28      0.92      0.43       146\n",
      "         5.0       1.00      0.35      0.52       148\n",
      "         6.0       0.48      0.24      0.32       122\n",
      "         7.0       0.52      0.57      0.54       120\n",
      "\n",
      "    accuracy                           0.45      1050\n",
      "   macro avg       0.57      0.46      0.44      1050\n",
      "weighted avg       0.58      0.45      0.43      1050\n",
      "\n",
      "[16,     3] loss: 0.129\n",
      "acc:0.4952 \n",
      "err:0.5048 \n",
      "[Validation RESULT]\n",
      "acc:0.4448 \n",
      "err:0.5552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.66      0.77       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.33      0.61      0.43       152\n",
      "         3.0       0.26      0.94      0.41       143\n",
      "         4.0       0.89      0.44      0.59       146\n",
      "         5.0       0.94      0.51      0.66       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       1.00      0.12      0.22       120\n",
      "\n",
      "    accuracy                           0.44      1050\n",
      "   macro avg       0.54      0.41      0.39      1050\n",
      "weighted avg       0.57      0.44      0.41      1050\n",
      "\n",
      "[17,     3] loss: 0.139\n",
      "acc:0.4825 \n",
      "err:0.5175 \n",
      "[Validation RESULT]\n",
      "acc:0.3962 \n",
      "err:0.6038\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.74      0.60       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.33      0.01      0.01       152\n",
      "         3.0       1.00      0.21      0.35       143\n",
      "         4.0       0.34      0.82      0.48       146\n",
      "         5.0       1.00      0.33      0.50       148\n",
      "         6.0       0.73      0.29      0.41       122\n",
      "         7.0       0.23      0.71      0.34       120\n",
      "\n",
      "    accuracy                           0.40      1050\n",
      "   macro avg       0.52      0.39      0.34      1050\n",
      "weighted avg       0.55      0.40      0.35      1050\n",
      "\n",
      "[18,     3] loss: 0.097\n",
      "acc:0.5429 \n",
      "err:0.4571 \n",
      "[Validation RESULT]\n",
      "acc:0.4714 \n",
      "err:0.5286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.53      0.70       129\n",
      "         1.0       0.18      0.99      0.30        90\n",
      "         2.0       0.64      0.50      0.56       152\n",
      "         3.0       0.45      0.52      0.48       143\n",
      "         4.0       0.98      0.41      0.58       146\n",
      "         5.0       0.96      0.50      0.66       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.91      0.44      0.60       120\n",
      "\n",
      "    accuracy                           0.47      1050\n",
      "   macro avg       0.64      0.49      0.48      1050\n",
      "weighted avg       0.67      0.47      0.50      1050\n",
      "\n",
      "[19,     3] loss: 0.108\n",
      "acc:0.5492 \n",
      "err:0.4508 \n",
      "[Validation RESULT]\n",
      "acc:0.4581 \n",
      "err:0.5419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.64      0.78       129\n",
      "         1.0       0.83      0.06      0.10        90\n",
      "         2.0       1.00      0.11      0.20       152\n",
      "         3.0       1.00      0.13      0.22       143\n",
      "         4.0       0.32      0.82      0.46       146\n",
      "         5.0       0.44      0.73      0.55       148\n",
      "         6.0       0.40      0.42      0.41       122\n",
      "         7.0       0.44      0.66      0.53       120\n",
      "\n",
      "    accuracy                           0.46      1050\n",
      "   macro avg       0.68      0.45      0.41      1050\n",
      "weighted avg       0.68      0.46      0.41      1050\n",
      "\n",
      "[20,     3] loss: 0.106\n",
      "acc:0.5460 \n",
      "err:0.4540 \n",
      "[Validation RESULT]\n",
      "acc:0.5114 \n",
      "err:0.4886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.37      0.36      0.36       152\n",
      "         3.0       0.54      0.67      0.60       143\n",
      "         4.0       0.32      0.91      0.48       146\n",
      "         5.0       1.00      0.24      0.38       148\n",
      "         6.0       0.59      0.56      0.57       122\n",
      "         7.0       0.85      0.55      0.67       120\n",
      "\n",
      "    accuracy                           0.51      1050\n",
      "   macro avg       0.58      0.49      0.48      1050\n",
      "weighted avg       0.60      0.51      0.49      1050\n",
      "\n",
      "[21,     3] loss: 0.114\n",
      "acc:0.5905 \n",
      "err:0.4095 \n",
      "[Validation RESULT]\n",
      "acc:0.4257 \n",
      "err:0.5743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.68      0.60       129\n",
      "         1.0       0.53      0.18      0.27        90\n",
      "         2.0       1.00      0.03      0.05       152\n",
      "         3.0       0.71      0.38      0.49       143\n",
      "         4.0       0.82      0.42      0.55       146\n",
      "         5.0       0.96      0.47      0.63       148\n",
      "         6.0       0.20      0.94      0.33       122\n",
      "         7.0       0.83      0.33      0.47       120\n",
      "\n",
      "    accuracy                           0.43      1050\n",
      "   macro avg       0.70      0.43      0.42      1050\n",
      "weighted avg       0.72      0.43      0.43      1050\n",
      "\n",
      "[22,     3] loss: 0.131\n",
      "acc:0.4857 \n",
      "err:0.5143 \n",
      "[Validation RESULT]\n",
      "acc:0.4867 \n",
      "err:0.5133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.57      0.73       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       0.97      0.40      0.57       152\n",
      "         3.0       0.36      0.84      0.50       143\n",
      "         4.0       0.94      0.42      0.58       146\n",
      "         5.0       0.73      0.61      0.66       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.27      0.88      0.41       120\n",
      "\n",
      "    accuracy                           0.49      1050\n",
      "   macro avg       0.53      0.46      0.43      1050\n",
      "weighted avg       0.58      0.49      0.46      1050\n",
      "\n",
      "[23,     3] loss: 0.083\n",
      "acc:0.5841 \n",
      "err:0.4159 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation RESULT]\n",
      "acc:0.4810 \n",
      "err:0.5190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.36      0.60      0.45        90\n",
      "         2.0       0.24      0.93      0.38       152\n",
      "         3.0       0.99      0.48      0.65       143\n",
      "         4.0       1.00      0.23      0.38       146\n",
      "         5.0       0.97      0.49      0.65       148\n",
      "         6.0       0.83      0.08      0.15       122\n",
      "         7.0       1.00      0.33      0.49       120\n",
      "\n",
      "    accuracy                           0.48      1050\n",
      "   macro avg       0.80      0.48      0.49      1050\n",
      "weighted avg       0.81      0.48      0.50      1050\n",
      "\n",
      "[24,     3] loss: 0.106\n",
      "acc:0.5651 \n",
      "err:0.4349 \n",
      "[Validation RESULT]\n",
      "acc:0.4933 \n",
      "err:0.5067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       1.00      0.04      0.08       152\n",
      "         3.0       0.97      0.40      0.56       143\n",
      "         4.0       0.41      0.73      0.53       146\n",
      "         5.0       0.65      0.66      0.65       148\n",
      "         6.0       0.27      0.89      0.41       122\n",
      "         7.0       0.66      0.48      0.56       120\n",
      "\n",
      "    accuracy                           0.49      1050\n",
      "   macro avg       0.62      0.48      0.45      1050\n",
      "weighted avg       0.65      0.49      0.46      1050\n",
      "\n",
      "[25,     3] loss: 0.131\n",
      "acc:0.4921 \n",
      "err:0.5079 \n",
      "[Validation RESULT]\n",
      "acc:0.4533 \n",
      "err:0.5467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.66      0.75       129\n",
      "         1.0       0.29      0.68      0.41        90\n",
      "         2.0       0.62      0.47      0.54       152\n",
      "         3.0       1.00      0.24      0.38       143\n",
      "         4.0       0.33      0.93      0.49       146\n",
      "         5.0       1.00      0.07      0.13       148\n",
      "         6.0       0.00      0.00      0.00       122\n",
      "         7.0       0.46      0.65      0.54       120\n",
      "\n",
      "    accuracy                           0.45      1050\n",
      "   macro avg       0.57      0.46      0.40      1050\n",
      "weighted avg       0.60      0.45      0.40      1050\n",
      "\n",
      "[26,     3] loss: 0.130\n",
      "acc:0.5619 \n",
      "err:0.4381 \n",
      "[Validation RESULT]\n",
      "acc:0.5229 \n",
      "err:0.4771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.65      0.79       129\n",
      "         1.0       1.00      0.03      0.06        90\n",
      "         2.0       0.25      0.96      0.40       152\n",
      "         3.0       0.97      0.48      0.64       143\n",
      "         4.0       1.00      0.32      0.48       146\n",
      "         5.0       0.88      0.50      0.64       148\n",
      "         6.0       0.62      0.66      0.64       122\n",
      "         7.0       1.00      0.39      0.56       120\n",
      "\n",
      "    accuracy                           0.52      1050\n",
      "   macro avg       0.84      0.50      0.53      1050\n",
      "weighted avg       0.83      0.52      0.54      1050\n",
      "\n",
      "[27,     3] loss: 0.058\n",
      "acc:0.5841 \n",
      "err:0.4159 \n",
      "[Validation RESULT]\n",
      "acc:0.5781 \n",
      "err:0.4219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.92      0.52       129\n",
      "         1.0       0.38      0.59      0.46        90\n",
      "         2.0       0.98      0.26      0.41       152\n",
      "         3.0       0.75      0.57      0.65       143\n",
      "         4.0       0.61      0.50      0.55       146\n",
      "         5.0       0.68      0.68      0.68       148\n",
      "         6.0       0.82      0.58      0.68       122\n",
      "         7.0       0.95      0.57      0.72       120\n",
      "\n",
      "    accuracy                           0.58      1050\n",
      "   macro avg       0.69      0.59      0.58      1050\n",
      "weighted avg       0.70      0.58      0.58      1050\n",
      "\n",
      "[28,     3] loss: 0.043\n",
      "acc:0.6952 \n",
      "err:0.3048 \n",
      "[Validation RESULT]\n",
      "acc:0.5771 \n",
      "err:0.4229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.50      0.02      0.04        90\n",
      "         2.0       0.98      0.39      0.56       152\n",
      "         3.0       0.41      0.77      0.53       143\n",
      "         4.0       0.66      0.62      0.64       146\n",
      "         5.0       0.50      0.68      0.57       148\n",
      "         6.0       0.45      0.64      0.53       122\n",
      "         7.0       0.70      0.67      0.68       120\n",
      "\n",
      "    accuracy                           0.58      1050\n",
      "   macro avg       0.65      0.56      0.54      1050\n",
      "weighted avg       0.66      0.58      0.56      1050\n",
      "\n",
      "[29,     3] loss: 0.062\n",
      "acc:0.5968 \n",
      "err:0.4032 \n",
      "[Validation RESULT]\n",
      "acc:0.5343 \n",
      "err:0.4657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.75      0.07      0.12        90\n",
      "         2.0       0.67      0.49      0.57       152\n",
      "         3.0       0.27      0.92      0.42       143\n",
      "         4.0       0.85      0.44      0.58       146\n",
      "         5.0       0.57      0.68      0.62       148\n",
      "         6.0       0.82      0.56      0.66       122\n",
      "         7.0       1.00      0.26      0.41       120\n",
      "\n",
      "    accuracy                           0.53      1050\n",
      "   macro avg       0.74      0.51      0.52      1050\n",
      "weighted avg       0.73      0.53      0.54      1050\n",
      "\n",
      "[30,     3] loss: 0.042\n",
      "acc:0.6730 \n",
      "err:0.3270 \n",
      "[Validation RESULT]\n",
      "acc:0.5952 \n",
      "err:0.4048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.81      0.69       129\n",
      "         1.0       0.50      0.18      0.26        90\n",
      "         2.0       0.50      0.61      0.55       152\n",
      "         3.0       0.99      0.49      0.65       143\n",
      "         4.0       0.55      0.62      0.59       146\n",
      "         5.0       0.72      0.59      0.65       148\n",
      "         6.0       0.42      0.82      0.56       122\n",
      "         7.0       0.98      0.53      0.69       120\n",
      "\n",
      "    accuracy                           0.60      1050\n",
      "   macro avg       0.66      0.58      0.58      1050\n",
      "weighted avg       0.66      0.60      0.59      1050\n",
      "\n",
      "[31,     3] loss: 0.028\n",
      "acc:0.7238 \n",
      "err:0.2762 \n",
      "[Validation RESULT]\n",
      "acc:0.6105 \n",
      "err:0.3895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.66      0.79       129\n",
      "         1.0       0.22      0.98      0.36        90\n",
      "         2.0       0.96      0.43      0.59       152\n",
      "         3.0       1.00      0.48      0.64       143\n",
      "         4.0       0.62      0.71      0.66       146\n",
      "         5.0       0.90      0.57      0.70       148\n",
      "         6.0       0.88      0.57      0.69       122\n",
      "         7.0       0.81      0.63      0.71       120\n",
      "\n",
      "    accuracy                           0.61      1050\n",
      "   macro avg       0.80      0.63      0.65      1050\n",
      "weighted avg       0.82      0.61      0.65      1050\n",
      "\n",
      "[32,     3] loss: 0.038\n",
      "acc:0.7111 \n",
      "err:0.2889 \n",
      "[Validation RESULT]\n",
      "acc:0.5800 \n",
      "err:0.4200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.95      0.51       129\n",
      "         1.0       0.75      0.03      0.06        90\n",
      "         2.0       0.49      0.63      0.55       152\n",
      "         3.0       0.88      0.52      0.66       143\n",
      "         4.0       0.68      0.58      0.63       146\n",
      "         5.0       0.69      0.61      0.65       148\n",
      "         6.0       0.94      0.51      0.66       122\n",
      "         7.0       0.84      0.63      0.72       120\n",
      "\n",
      "    accuracy                           0.58      1050\n",
      "   macro avg       0.70      0.56      0.55      1050\n",
      "weighted avg       0.69      0.58      0.57      1050\n",
      "\n",
      "[33,     3] loss: 0.020\n",
      "acc:0.7524 \n",
      "err:0.2476 \n",
      "[Validation RESULT]\n",
      "acc:0.6362 \n",
      "err:0.3638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.43      0.48      0.46        90\n",
      "         2.0       0.95      0.47      0.63       152\n",
      "         3.0       0.58      0.63      0.61       143\n",
      "         4.0       0.62      0.71      0.66       146\n",
      "         5.0       0.47      0.76      0.58       148\n",
      "         6.0       0.82      0.63      0.71       122\n",
      "         7.0       0.63      0.72      0.67       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.69      0.63      0.64      1050\n",
      "weighted avg       0.70      0.64      0.64      1050\n",
      "\n",
      "[34,     3] loss: 0.015\n",
      "acc:0.7873 \n",
      "err:0.2127 \n",
      "[Validation RESULT]\n",
      "acc:0.6019 \n",
      "err:0.3981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       129\n",
      "         1.0       0.39      0.56      0.46        90\n",
      "         2.0       0.37      0.64      0.47       152\n",
      "         3.0       0.91      0.50      0.65       143\n",
      "         4.0       0.52      0.71      0.60       146\n",
      "         5.0       0.95      0.49      0.64       148\n",
      "         6.0       0.57      0.73      0.64       122\n",
      "         7.0       0.98      0.53      0.68       120\n",
      "\n",
      "    accuracy                           0.60      1050\n",
      "   macro avg       0.71      0.60      0.62      1050\n",
      "weighted avg       0.72      0.60      0.62      1050\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,     3] loss: 0.019\n",
      "acc:0.7302 \n",
      "err:0.2698 \n",
      "[Validation RESULT]\n",
      "acc:0.6114 \n",
      "err:0.3886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.44      0.24      0.31        90\n",
      "         2.0       0.57      0.61      0.59       152\n",
      "         3.0       0.35      0.86      0.50       143\n",
      "         4.0       0.70      0.63      0.66       146\n",
      "         5.0       0.91      0.55      0.68       148\n",
      "         6.0       1.00      0.52      0.68       122\n",
      "         7.0       0.69      0.70      0.70       120\n",
      "\n",
      "    accuracy                           0.61      1050\n",
      "   macro avg       0.71      0.60      0.62      1050\n",
      "weighted avg       0.71      0.61      0.63      1050\n",
      "\n",
      "[36,     3] loss: 0.014\n",
      "acc:0.8095 \n",
      "err:0.1905 \n",
      "[Validation RESULT]\n",
      "acc:0.6619 \n",
      "err:0.3381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.78      0.75       129\n",
      "         1.0       0.46      0.67      0.54        90\n",
      "         2.0       0.83      0.53      0.65       152\n",
      "         3.0       0.93      0.53      0.68       143\n",
      "         4.0       0.76      0.59      0.66       146\n",
      "         5.0       0.46      0.83      0.59       148\n",
      "         6.0       0.69      0.75      0.72       122\n",
      "         7.0       0.92      0.64      0.75       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.72      0.67      0.67      1050\n",
      "weighted avg       0.73      0.66      0.67      1050\n",
      "\n",
      "[37,     3] loss: 0.013\n",
      "acc:0.8000 \n",
      "err:0.2000 \n",
      "[Validation RESULT]\n",
      "acc:0.6581 \n",
      "err:0.3419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.68      0.78       129\n",
      "         1.0       0.40      0.79      0.53        90\n",
      "         2.0       0.69      0.57      0.62       152\n",
      "         3.0       0.60      0.64      0.62       143\n",
      "         4.0       0.70      0.66      0.68       146\n",
      "         5.0       0.82      0.61      0.70       148\n",
      "         6.0       0.99      0.54      0.70       122\n",
      "         7.0       0.56      0.83      0.67       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.71      0.67      0.66      1050\n",
      "weighted avg       0.72      0.66      0.67      1050\n",
      "\n",
      "[38,     3] loss: 0.012\n",
      "acc:0.8190 \n",
      "err:0.1810 \n",
      "[Validation RESULT]\n",
      "acc:0.6619 \n",
      "err:0.3381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.69      0.78       129\n",
      "         1.0       0.46      0.77      0.57        90\n",
      "         2.0       0.55      0.67      0.61       152\n",
      "         3.0       0.54      0.69      0.60       143\n",
      "         4.0       0.75      0.57      0.65       146\n",
      "         5.0       0.70      0.63      0.66       148\n",
      "         6.0       0.82      0.67      0.74       122\n",
      "         7.0       0.86      0.66      0.75       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.70      0.67      0.67      1050\n",
      "weighted avg       0.70      0.66      0.67      1050\n",
      "\n",
      "[39,     3] loss: 0.011\n",
      "acc:0.8222 \n",
      "err:0.1778 \n",
      "[Validation RESULT]\n",
      "acc:0.6552 \n",
      "err:0.3448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.68      0.77       129\n",
      "         1.0       0.38      0.93      0.54        90\n",
      "         2.0       0.80      0.51      0.62       152\n",
      "         3.0       0.84      0.55      0.67       143\n",
      "         4.0       0.74      0.62      0.67       146\n",
      "         5.0       0.52      0.75      0.62       148\n",
      "         6.0       0.95      0.61      0.75       122\n",
      "         7.0       0.67      0.69      0.68       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.72      0.67      0.66      1050\n",
      "weighted avg       0.73      0.66      0.67      1050\n",
      "\n",
      "[40,     3] loss: 0.012\n",
      "acc:0.8317 \n",
      "err:0.1683 \n",
      "[Validation RESULT]\n",
      "acc:0.6638 \n",
      "err:0.3362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.73      0.76       129\n",
      "         1.0       0.37      0.84      0.51        90\n",
      "         2.0       0.76      0.53      0.63       152\n",
      "         3.0       0.88      0.52      0.65       143\n",
      "         4.0       0.63      0.72      0.67       146\n",
      "         5.0       0.83      0.58      0.68       148\n",
      "         6.0       0.88      0.65      0.75       122\n",
      "         7.0       0.58      0.85      0.69       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.72      0.68      0.67      1050\n",
      "weighted avg       0.73      0.66      0.67      1050\n",
      "\n",
      "[41,     3] loss: 0.013\n",
      "acc:0.8190 \n",
      "err:0.1810 \n",
      "[Validation RESULT]\n",
      "acc:0.6476 \n",
      "err:0.3524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.80      0.73       129\n",
      "         1.0       0.54      0.48      0.51        90\n",
      "         2.0       0.55      0.58      0.57       152\n",
      "         3.0       0.63      0.66      0.64       143\n",
      "         4.0       0.47      0.81      0.59       146\n",
      "         5.0       0.95      0.52      0.67       148\n",
      "         6.0       0.88      0.66      0.76       122\n",
      "         7.0       0.92      0.63      0.75       120\n",
      "\n",
      "    accuracy                           0.65      1050\n",
      "   macro avg       0.70      0.64      0.65      1050\n",
      "weighted avg       0.70      0.65      0.65      1050\n",
      "\n",
      "[42,     3] loss: 0.018\n",
      "acc:0.7587 \n",
      "err:0.2413 \n",
      "[Validation RESULT]\n",
      "acc:0.5838 \n",
      "err:0.4162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.66      0.79       129\n",
      "         1.0       1.00      0.01      0.02        90\n",
      "         2.0       0.47      0.59      0.52       152\n",
      "         3.0       0.76      0.55      0.63       143\n",
      "         4.0       0.44      0.84      0.58       146\n",
      "         5.0       0.43      0.72      0.54       148\n",
      "         6.0       1.00      0.48      0.64       122\n",
      "         7.0       0.89      0.61      0.72       120\n",
      "\n",
      "    accuracy                           0.58      1050\n",
      "   macro avg       0.74      0.56      0.56      1050\n",
      "weighted avg       0.72      0.58      0.57      1050\n",
      "\n",
      "[43,     3] loss: 0.020\n",
      "acc:0.7492 \n",
      "err:0.2508 \n",
      "[Validation RESULT]\n",
      "acc:0.6086 \n",
      "err:0.3914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.67      0.77       129\n",
      "         1.0       0.51      0.39      0.44        90\n",
      "         2.0       0.77      0.54      0.63       152\n",
      "         3.0       0.31      0.92      0.47       143\n",
      "         4.0       0.78      0.55      0.65       146\n",
      "         5.0       0.82      0.64      0.71       148\n",
      "         6.0       1.00      0.45      0.62       122\n",
      "         7.0       0.85      0.62      0.72       120\n",
      "\n",
      "    accuracy                           0.61      1050\n",
      "   macro avg       0.74      0.60      0.63      1050\n",
      "weighted avg       0.75      0.61      0.63      1050\n",
      "\n",
      "[44,     3] loss: 0.015\n",
      "acc:0.7810 \n",
      "err:0.2190 \n",
      "[Validation RESULT]\n",
      "acc:0.6390 \n",
      "err:0.3610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.38      0.83      0.52        90\n",
      "         2.0       0.94      0.44      0.60       152\n",
      "         3.0       1.00      0.48      0.64       143\n",
      "         4.0       0.39      0.95      0.55       146\n",
      "         5.0       0.83      0.58      0.68       148\n",
      "         6.0       0.84      0.66      0.74       122\n",
      "         7.0       0.99      0.58      0.73       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.80      0.65      0.66      1050\n",
      "weighted avg       0.81      0.64      0.66      1050\n",
      "\n",
      "[45,     3] loss: 0.021\n",
      "acc:0.7175 \n",
      "err:0.2825 \n",
      "[Validation RESULT]\n",
      "acc:0.6552 \n",
      "err:0.3448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.74      0.78       129\n",
      "         1.0       0.32      0.96      0.48        90\n",
      "         2.0       0.65      0.57      0.61       152\n",
      "         3.0       0.95      0.52      0.67       143\n",
      "         4.0       0.75      0.62      0.68       146\n",
      "         5.0       0.90      0.53      0.66       148\n",
      "         6.0       0.62      0.75      0.68       122\n",
      "         7.0       0.89      0.71      0.79       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.74      0.67      0.67      1050\n",
      "weighted avg       0.76      0.66      0.67      1050\n",
      "\n",
      "[46,     3] loss: 0.012\n",
      "acc:0.8000 \n",
      "err:0.2000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation RESULT]\n",
      "acc:0.6695 \n",
      "err:0.3305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.66      0.79       129\n",
      "         1.0       0.45      0.60      0.52        90\n",
      "         2.0       0.70      0.58      0.64       152\n",
      "         3.0       0.60      0.71      0.65       143\n",
      "         4.0       0.60      0.76      0.67       146\n",
      "         5.0       0.81      0.59      0.68       148\n",
      "         6.0       0.94      0.61      0.74       122\n",
      "         7.0       0.57      0.84      0.68       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.71      0.67      0.67      1050\n",
      "weighted avg       0.71      0.67      0.68      1050\n",
      "\n",
      "[47,     3] loss: 0.013\n",
      "acc:0.8190 \n",
      "err:0.1810 \n",
      "[Validation RESULT]\n",
      "acc:0.6333 \n",
      "err:0.3667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.74      0.74       129\n",
      "         1.0       0.45      0.06      0.10        90\n",
      "         2.0       0.75      0.57      0.65       152\n",
      "         3.0       0.91      0.52      0.66       143\n",
      "         4.0       0.55      0.77      0.65       146\n",
      "         5.0       0.76      0.66      0.70       148\n",
      "         6.0       0.40      0.84      0.55       122\n",
      "         7.0       0.71      0.76      0.73       120\n",
      "\n",
      "    accuracy                           0.63      1050\n",
      "   macro avg       0.66      0.61      0.60      1050\n",
      "weighted avg       0.68      0.63      0.62      1050\n",
      "\n",
      "[48,     3] loss: 0.014\n",
      "acc:0.8317 \n",
      "err:0.1683 \n",
      "[Validation RESULT]\n",
      "acc:0.6581 \n",
      "err:0.3419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.67      0.79       129\n",
      "         1.0       0.47      0.68      0.56        90\n",
      "         2.0       0.80      0.53      0.64       152\n",
      "         3.0       0.71      0.57      0.64       143\n",
      "         4.0       0.58      0.76      0.65       146\n",
      "         5.0       0.79      0.63      0.70       148\n",
      "         6.0       0.95      0.61      0.75       122\n",
      "         7.0       0.45      0.85      0.59       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.71      0.66      0.66      1050\n",
      "weighted avg       0.73      0.66      0.67      1050\n",
      "\n",
      "[49,     3] loss: 0.011\n",
      "acc:0.8254 \n",
      "err:0.1746 \n",
      "[Validation RESULT]\n",
      "acc:0.6381 \n",
      "err:0.3619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.37      0.61      0.46        90\n",
      "         2.0       0.47      0.69      0.56       152\n",
      "         3.0       0.47      0.78      0.59       143\n",
      "         4.0       0.74      0.58      0.65       146\n",
      "         5.0       0.93      0.54      0.68       148\n",
      "         6.0       0.96      0.61      0.75       122\n",
      "         7.0       0.94      0.62      0.75       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.74      0.64      0.65      1050\n",
      "weighted avg       0.74      0.64      0.66      1050\n",
      "\n",
      "[50,     3] loss: 0.011\n",
      "acc:0.8635 \n",
      "err:0.1365 \n",
      "[Validation RESULT]\n",
      "acc:0.6648 \n",
      "err:0.3352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.78      0.70       129\n",
      "         1.0       0.60      0.48      0.53        90\n",
      "         2.0       0.92      0.51      0.65       152\n",
      "         3.0       0.57      0.69      0.62       143\n",
      "         4.0       0.63      0.71      0.67       146\n",
      "         5.0       0.56      0.74      0.64       148\n",
      "         6.0       0.86      0.66      0.74       122\n",
      "         7.0       0.79      0.71      0.75       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.69      0.66      0.66      1050\n",
      "weighted avg       0.70      0.66      0.67      1050\n",
      "\n",
      "[51,     3] loss: 0.009\n",
      "acc:0.8730 \n",
      "err:0.1270 \n",
      "[Validation RESULT]\n",
      "acc:0.6410 \n",
      "err:0.3590\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.75      0.74       129\n",
      "         1.0       0.66      0.23      0.34        90\n",
      "         2.0       0.70      0.57      0.63       152\n",
      "         3.0       0.46      0.81      0.58       143\n",
      "         4.0       0.56      0.81      0.66       146\n",
      "         5.0       0.75      0.57      0.65       148\n",
      "         6.0       0.98      0.52      0.68       122\n",
      "         7.0       0.72      0.71      0.71       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.69      0.62      0.63      1050\n",
      "weighted avg       0.69      0.64      0.64      1050\n",
      "\n",
      "[52,     3] loss: 0.009\n",
      "acc:0.8603 \n",
      "err:0.1397 \n",
      "[Validation RESULT]\n",
      "acc:0.6590 \n",
      "err:0.3410\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.75      0.71       129\n",
      "         1.0       0.56      0.49      0.52        90\n",
      "         2.0       0.70      0.59      0.64       152\n",
      "         3.0       0.52      0.76      0.61       143\n",
      "         4.0       0.69      0.68      0.68       146\n",
      "         5.0       0.56      0.69      0.62       148\n",
      "         6.0       0.93      0.65      0.76       122\n",
      "         7.0       0.94      0.62      0.74       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.69      0.65      0.66      1050\n",
      "weighted avg       0.69      0.66      0.66      1050\n",
      "\n",
      "[53,     3] loss: 0.008\n",
      "acc:0.8762 \n",
      "err:0.1238 \n",
      "[Validation RESULT]\n",
      "acc:0.6676 \n",
      "err:0.3324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.67      0.79       129\n",
      "         1.0       0.42      0.77      0.54        90\n",
      "         2.0       0.55      0.67      0.60       152\n",
      "         3.0       0.67      0.63      0.65       143\n",
      "         4.0       0.75      0.68      0.71       146\n",
      "         5.0       0.74      0.64      0.69       148\n",
      "         6.0       0.95      0.59      0.73       122\n",
      "         7.0       0.64      0.72      0.68       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.71      0.67      0.67      1050\n",
      "weighted avg       0.71      0.67      0.68      1050\n",
      "\n",
      "[54,     3] loss: 0.008\n",
      "acc:0.8794 \n",
      "err:0.1206 \n",
      "[Validation RESULT]\n",
      "acc:0.6629 \n",
      "err:0.3371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       129\n",
      "         1.0       0.53      0.47      0.49        90\n",
      "         2.0       0.78      0.56      0.65       152\n",
      "         3.0       0.73      0.60      0.66       143\n",
      "         4.0       0.55      0.80      0.65       146\n",
      "         5.0       0.83      0.56      0.67       148\n",
      "         6.0       0.70      0.71      0.70       122\n",
      "         7.0       0.54      0.82      0.65       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.68      0.66      0.66      1050\n",
      "weighted avg       0.69      0.66      0.66      1050\n",
      "\n",
      "[55,     3] loss: 0.008\n",
      "acc:0.8794 \n",
      "err:0.1206 \n",
      "[Validation RESULT]\n",
      "acc:0.6733 \n",
      "err:0.3267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.74      0.76       129\n",
      "         1.0       0.54      0.56      0.55        90\n",
      "         2.0       0.59      0.63      0.61       152\n",
      "         3.0       0.69      0.64      0.66       143\n",
      "         4.0       0.65      0.72      0.68       146\n",
      "         5.0       0.64      0.68      0.66       148\n",
      "         6.0       0.86      0.69      0.76       122\n",
      "         7.0       0.70      0.71      0.71       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.68      0.67      0.67      1050\n",
      "weighted avg       0.68      0.67      0.67      1050\n",
      "\n",
      "[56,     3] loss: 0.009\n",
      "acc:0.8730 \n",
      "err:0.1270 \n",
      "[Validation RESULT]\n",
      "acc:0.6276 \n",
      "err:0.3724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.66      0.79       129\n",
      "         1.0       0.66      0.32      0.43        90\n",
      "         2.0       0.77      0.57      0.65       152\n",
      "         3.0       0.32      0.93      0.48       143\n",
      "         4.0       0.76      0.59      0.66       146\n",
      "         5.0       0.73      0.64      0.68       148\n",
      "         6.0       0.96      0.58      0.72       122\n",
      "         7.0       0.93      0.62      0.75       120\n",
      "\n",
      "    accuracy                           0.63      1050\n",
      "   macro avg       0.76      0.61      0.65      1050\n",
      "weighted avg       0.76      0.63      0.65      1050\n",
      "\n",
      "[57,     3] loss: 0.010\n",
      "acc:0.8667 \n",
      "err:0.1333 \n",
      "[Validation RESULT]\n",
      "acc:0.6390 \n",
      "err:0.3610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.74      0.75       129\n",
      "         1.0       0.59      0.19      0.29        90\n",
      "         2.0       0.76      0.58      0.66       152\n",
      "         3.0       0.95      0.52      0.67       143\n",
      "         4.0       0.59      0.75      0.66       146\n",
      "         5.0       0.50      0.71      0.59       148\n",
      "         6.0       0.49      0.85      0.62       122\n",
      "         7.0       0.81      0.66      0.73       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.68      0.62      0.62      1050\n",
      "weighted avg       0.69      0.64      0.63      1050\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58,     3] loss: 0.014\n",
      "acc:0.7778 \n",
      "err:0.2222 \n",
      "[Validation RESULT]\n",
      "acc:0.6152 \n",
      "err:0.3848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.85      0.59       129\n",
      "         1.0       0.33      0.73      0.46        90\n",
      "         2.0       0.71      0.60      0.65       152\n",
      "         3.0       0.74      0.55      0.63       143\n",
      "         4.0       0.97      0.44      0.60       146\n",
      "         5.0       0.63      0.72      0.67       148\n",
      "         6.0       0.98      0.52      0.68       122\n",
      "         7.0       0.99      0.55      0.71       120\n",
      "\n",
      "    accuracy                           0.62      1050\n",
      "   macro avg       0.72      0.62      0.62      1050\n",
      "weighted avg       0.74      0.62      0.63      1050\n",
      "\n",
      "[59,     3] loss: 0.026\n",
      "acc:0.7873 \n",
      "err:0.2127 \n",
      "[Validation RESULT]\n",
      "acc:0.5857 \n",
      "err:0.4143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.66      0.79       129\n",
      "         1.0       0.45      0.28      0.34        90\n",
      "         2.0       0.43      0.75      0.55       152\n",
      "         3.0       1.00      0.40      0.57       143\n",
      "         4.0       0.97      0.40      0.56       146\n",
      "         5.0       0.77      0.60      0.68       148\n",
      "         6.0       0.34      0.94      0.50       122\n",
      "         7.0       0.97      0.60      0.74       120\n",
      "\n",
      "    accuracy                           0.59      1050\n",
      "   macro avg       0.74      0.58      0.59      1050\n",
      "weighted avg       0.75      0.59      0.60      1050\n",
      "\n",
      "[60,     3] loss: 0.026\n",
      "acc:0.7302 \n",
      "err:0.2698 \n",
      "[Validation RESULT]\n",
      "acc:0.5714 \n",
      "err:0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.66      0.79       129\n",
      "         1.0       0.62      0.06      0.10        90\n",
      "         2.0       0.92      0.29      0.44       152\n",
      "         3.0       0.40      0.85      0.54       143\n",
      "         4.0       0.37      0.92      0.53       146\n",
      "         5.0       0.90      0.54      0.68       148\n",
      "         6.0       0.98      0.43      0.59       122\n",
      "         7.0       0.80      0.66      0.72       120\n",
      "\n",
      "    accuracy                           0.57      1050\n",
      "   macro avg       0.75      0.55      0.55      1050\n",
      "weighted avg       0.74      0.57      0.56      1050\n",
      "\n",
      "[61,     3] loss: 0.042\n",
      "acc:0.7143 \n",
      "err:0.2857 \n",
      "[Validation RESULT]\n",
      "acc:0.5857 \n",
      "err:0.4143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.50      0.06      0.10        90\n",
      "         2.0       0.46      0.75      0.57       152\n",
      "         3.0       0.32      0.92      0.48       143\n",
      "         4.0       0.89      0.46      0.61       146\n",
      "         5.0       0.97      0.51      0.67       148\n",
      "         6.0       0.89      0.65      0.75       122\n",
      "         7.0       1.00      0.49      0.66       120\n",
      "\n",
      "    accuracy                           0.59      1050\n",
      "   macro avg       0.75      0.56      0.58      1050\n",
      "weighted avg       0.75      0.59      0.59      1050\n",
      "\n",
      "[62,     3] loss: 0.043\n",
      "acc:0.6857 \n",
      "err:0.3143 \n",
      "[Validation RESULT]\n",
      "acc:0.5552 \n",
      "err:0.4448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.66      0.78       129\n",
      "         1.0       1.00      0.01      0.02        90\n",
      "         2.0       0.28      0.95      0.43       152\n",
      "         3.0       0.95      0.52      0.67       143\n",
      "         4.0       0.97      0.43      0.60       146\n",
      "         5.0       0.58      0.72      0.65       148\n",
      "         6.0       1.00      0.34      0.51       122\n",
      "         7.0       0.91      0.56      0.69       120\n",
      "\n",
      "    accuracy                           0.56      1050\n",
      "   macro avg       0.83      0.52      0.54      1050\n",
      "weighted avg       0.81      0.56      0.56      1050\n",
      "\n",
      "[63,     3] loss: 0.031\n",
      "acc:0.7143 \n",
      "err:0.2857 \n",
      "[Validation RESULT]\n",
      "acc:0.6390 \n",
      "err:0.3610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.66      0.78       129\n",
      "         1.0       0.33      0.90      0.49        90\n",
      "         2.0       0.93      0.45      0.60       152\n",
      "         3.0       0.87      0.52      0.66       143\n",
      "         4.0       0.66      0.75      0.70       146\n",
      "         5.0       0.58      0.67      0.62       148\n",
      "         6.0       0.97      0.51      0.67       122\n",
      "         7.0       0.57      0.77      0.65       120\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.74      0.65      0.65      1050\n",
      "weighted avg       0.75      0.64      0.65      1050\n",
      "\n",
      "[64,     3] loss: 0.019\n",
      "acc:0.8032 \n",
      "err:0.1968 \n",
      "[Validation RESULT]\n",
      "acc:0.5743 \n",
      "err:0.4257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.98      0.43       129\n",
      "         1.0       0.40      0.36      0.38        90\n",
      "         2.0       0.63      0.61      0.62       152\n",
      "         3.0       0.99      0.50      0.67       143\n",
      "         4.0       0.96      0.46      0.62       146\n",
      "         5.0       1.00      0.49      0.66       148\n",
      "         6.0       0.89      0.57      0.70       122\n",
      "         7.0       0.99      0.57      0.73       120\n",
      "\n",
      "    accuracy                           0.57      1050\n",
      "   macro avg       0.77      0.57      0.60      1050\n",
      "weighted avg       0.78      0.57      0.61      1050\n",
      "\n",
      "[65,     3] loss: 0.033\n",
      "acc:0.7524 \n",
      "err:0.2476 \n",
      "[Validation RESULT]\n",
      "acc:0.5324 \n",
      "err:0.4676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.61      0.76       129\n",
      "         1.0       0.53      0.11      0.18        90\n",
      "         2.0       0.90      0.41      0.56       152\n",
      "         3.0       1.00      0.46      0.63       143\n",
      "         4.0       0.56      0.61      0.58       146\n",
      "         5.0       0.86      0.55      0.67       148\n",
      "         6.0       0.89      0.52      0.66       122\n",
      "         7.0       0.22      0.90      0.35       120\n",
      "\n",
      "    accuracy                           0.53      1050\n",
      "   macro avg       0.74      0.52      0.55      1050\n",
      "weighted avg       0.76      0.53      0.57      1050\n",
      "\n",
      "[66,     3] loss: 0.047\n",
      "acc:0.6635 \n",
      "err:0.3365 \n",
      "[Validation RESULT]\n",
      "acc:0.5267 \n",
      "err:0.4733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.66      0.78       129\n",
      "         1.0       0.00      0.00      0.00        90\n",
      "         2.0       1.00      0.26      0.42       152\n",
      "         3.0       0.48      0.58      0.52       143\n",
      "         4.0       0.82      0.48      0.61       146\n",
      "         5.0       0.28      0.94      0.44       148\n",
      "         6.0       0.98      0.47      0.63       122\n",
      "         7.0       0.69      0.66      0.67       120\n",
      "\n",
      "    accuracy                           0.53      1050\n",
      "   macro avg       0.65      0.51      0.51      1050\n",
      "weighted avg       0.68      0.53      0.52      1050\n",
      "\n",
      "[67,     3] loss: 0.056\n",
      "acc:0.6444 \n",
      "err:0.3556 \n",
      "[Validation RESULT]\n",
      "acc:0.5248 \n",
      "err:0.4752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.66      0.71       129\n",
      "         1.0       0.18      1.00      0.31        90\n",
      "         2.0       0.92      0.51      0.66       152\n",
      "         3.0       1.00      0.46      0.63       143\n",
      "         4.0       0.84      0.45      0.58       146\n",
      "         5.0       0.94      0.53      0.68       148\n",
      "         6.0       1.00      0.07      0.14       122\n",
      "         7.0       0.63      0.66      0.64       120\n",
      "\n",
      "    accuracy                           0.52      1050\n",
      "   macro avg       0.78      0.54      0.54      1050\n",
      "weighted avg       0.82      0.52      0.56      1050\n",
      "\n",
      "[68,     3] loss: 0.069\n",
      "acc:0.6381 \n",
      "err:0.3619 \n",
      "[Validation RESULT]\n",
      "acc:0.6181 \n",
      "err:0.3819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       129\n",
      "         1.0       0.52      0.31      0.39        90\n",
      "         2.0       0.69      0.57      0.62       152\n",
      "         3.0       0.46      0.83      0.59       143\n",
      "         4.0       0.42      0.84      0.56       146\n",
      "         5.0       0.94      0.52      0.67       148\n",
      "         6.0       0.78      0.62      0.69       122\n",
      "         7.0       1.00      0.46      0.63       120\n",
      "\n",
      "    accuracy                           0.62      1050\n",
      "   macro avg       0.73      0.60      0.62      1050\n",
      "weighted avg       0.73      0.62      0.63      1050\n",
      "\n",
      "[69,     3] loss: 0.020\n",
      "acc:0.7746 \n",
      "err:0.2254 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation RESULT]\n",
      "acc:0.6495 \n",
      "err:0.3505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.88      0.63       129\n",
      "         1.0       0.50      0.57      0.53        90\n",
      "         2.0       0.77      0.55      0.64       152\n",
      "         3.0       0.93      0.52      0.67       143\n",
      "         4.0       0.47      0.83      0.60       146\n",
      "         5.0       0.83      0.61      0.70       148\n",
      "         6.0       0.96      0.58      0.72       122\n",
      "         7.0       0.86      0.64      0.73       120\n",
      "\n",
      "    accuracy                           0.65      1050\n",
      "   macro avg       0.73      0.65      0.65      1050\n",
      "weighted avg       0.73      0.65      0.66      1050\n",
      "\n",
      "[70,     3] loss: 0.016\n",
      "acc:0.7905 \n",
      "err:0.2095 \n",
      "[Validation RESULT]\n",
      "acc:0.6600 \n",
      "err:0.3400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.66      0.79       129\n",
      "         1.0       0.42      0.73      0.53        90\n",
      "         2.0       0.63      0.62      0.63       152\n",
      "         3.0       0.65      0.65      0.65       143\n",
      "         4.0       0.70      0.64      0.67       146\n",
      "         5.0       0.56      0.68      0.61       148\n",
      "         6.0       0.73      0.69      0.71       122\n",
      "         7.0       0.88      0.64      0.74       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.70      0.66      0.67      1050\n",
      "weighted avg       0.70      0.66      0.67      1050\n",
      "\n",
      "[71,     3] loss: 0.010\n",
      "acc:0.8444 \n",
      "err:0.1556 \n",
      "[Validation RESULT]\n",
      "acc:0.6476 \n",
      "err:0.3524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.71      0.74       129\n",
      "         1.0       0.47      0.63      0.54        90\n",
      "         2.0       0.53      0.62      0.58       152\n",
      "         3.0       0.60      0.68      0.64       143\n",
      "         4.0       0.52      0.80      0.63       146\n",
      "         5.0       0.83      0.57      0.67       148\n",
      "         6.0       0.98      0.52      0.68       122\n",
      "         7.0       0.92      0.63      0.75       120\n",
      "\n",
      "    accuracy                           0.65      1050\n",
      "   macro avg       0.70      0.65      0.65      1050\n",
      "weighted avg       0.70      0.65      0.65      1050\n",
      "\n",
      "[72,     3] loss: 0.013\n",
      "acc:0.8603 \n",
      "err:0.1397 \n",
      "[Validation RESULT]\n",
      "acc:0.6124 \n",
      "err:0.3876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.73      0.71       129\n",
      "         1.0       0.56      0.21      0.31        90\n",
      "         2.0       0.60      0.64      0.62       152\n",
      "         3.0       0.40      0.78      0.53       143\n",
      "         4.0       0.92      0.46      0.61       146\n",
      "         5.0       0.50      0.72      0.59       148\n",
      "         6.0       0.96      0.57      0.72       122\n",
      "         7.0       0.96      0.63      0.76       120\n",
      "\n",
      "    accuracy                           0.61      1050\n",
      "   macro avg       0.70      0.59      0.61      1050\n",
      "weighted avg       0.69      0.61      0.61      1050\n",
      "\n",
      "[73,     3] loss: 0.013\n",
      "acc:0.8063 \n",
      "err:0.1937 \n",
      "[Validation RESULT]\n",
      "acc:0.6552 \n",
      "err:0.3448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.75      0.72       129\n",
      "         1.0       0.54      0.34      0.42        90\n",
      "         2.0       0.60      0.64      0.62       152\n",
      "         3.0       0.48      0.76      0.59       143\n",
      "         4.0       0.66      0.66      0.66       146\n",
      "         5.0       0.78      0.63      0.70       148\n",
      "         6.0       0.73      0.70      0.72       122\n",
      "         7.0       0.96      0.66      0.78       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.68      0.64      0.65      1050\n",
      "weighted avg       0.68      0.66      0.66      1050\n",
      "\n",
      "[74,     3] loss: 0.008\n",
      "acc:0.8857 \n",
      "err:0.1143 \n",
      "[Validation RESULT]\n",
      "acc:0.6790 \n",
      "err:0.3210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.69      0.81       129\n",
      "         1.0       0.38      0.87      0.53        90\n",
      "         2.0       0.61      0.61      0.61       152\n",
      "         3.0       0.88      0.55      0.67       143\n",
      "         4.0       0.69      0.73      0.71       146\n",
      "         5.0       0.81      0.62      0.70       148\n",
      "         6.0       0.87      0.66      0.75       122\n",
      "         7.0       0.64      0.81      0.72       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.73      0.69      0.69      1050\n",
      "weighted avg       0.74      0.68      0.69      1050\n",
      "\n",
      "[75,     3] loss: 0.007\n",
      "acc:0.9143 \n",
      "err:0.0857 \n",
      "[Validation RESULT]\n",
      "acc:0.6657 \n",
      "err:0.3343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.70      0.77       129\n",
      "         1.0       0.60      0.46      0.52        90\n",
      "         2.0       0.66      0.62      0.64       152\n",
      "         3.0       0.45      0.78      0.57       143\n",
      "         4.0       0.69      0.67      0.68       146\n",
      "         5.0       0.65      0.67      0.66       148\n",
      "         6.0       0.83      0.70      0.76       122\n",
      "         7.0       0.90      0.68      0.77       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.70      0.66      0.67      1050\n",
      "weighted avg       0.70      0.67      0.67      1050\n",
      "\n",
      "[76,     3] loss: 0.007\n",
      "acc:0.9302 \n",
      "err:0.0698 \n",
      "[Validation RESULT]\n",
      "acc:0.6733 \n",
      "err:0.3267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.74      0.73       129\n",
      "         1.0       0.54      0.57      0.55        90\n",
      "         2.0       0.71      0.59      0.65       152\n",
      "         3.0       0.68      0.62      0.65       143\n",
      "         4.0       0.70      0.68      0.69       146\n",
      "         5.0       0.55      0.71      0.62       148\n",
      "         6.0       0.76      0.71      0.73       122\n",
      "         7.0       0.78      0.75      0.76       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.68      0.67      0.67      1050\n",
      "weighted avg       0.68      0.67      0.67      1050\n",
      "\n",
      "[77,     3] loss: 0.007\n",
      "acc:0.9048 \n",
      "err:0.0952 \n",
      "[Validation RESULT]\n",
      "acc:0.6810 \n",
      "err:0.3190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.74      0.79       129\n",
      "         1.0       0.43      0.79      0.56        90\n",
      "         2.0       0.62      0.59      0.60       152\n",
      "         3.0       0.84      0.57      0.68       143\n",
      "         4.0       0.63      0.75      0.69       146\n",
      "         5.0       0.85      0.59      0.70       148\n",
      "         6.0       0.83      0.69      0.75       122\n",
      "         7.0       0.64      0.80      0.71       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.71      0.69      0.68      1050\n",
      "weighted avg       0.72      0.68      0.69      1050\n",
      "\n",
      "[78,     3] loss: 0.007\n",
      "acc:0.9302 \n",
      "err:0.0698 \n",
      "[Validation RESULT]\n",
      "acc:0.6762 \n",
      "err:0.3238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.72      0.80       129\n",
      "         1.0       0.49      0.54      0.52        90\n",
      "         2.0       0.53      0.66      0.59       152\n",
      "         3.0       0.70      0.61      0.65       143\n",
      "         4.0       0.64      0.75      0.69       146\n",
      "         5.0       0.79      0.61      0.69       148\n",
      "         6.0       0.66      0.76      0.71       122\n",
      "         7.0       0.81      0.73      0.77       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.69      0.67      0.68      1050\n",
      "weighted avg       0.70      0.68      0.68      1050\n",
      "\n",
      "[79,     3] loss: 0.006\n",
      "acc:0.9111 \n",
      "err:0.0889 \n",
      "[Validation RESULT]\n",
      "acc:0.6771 \n",
      "err:0.3229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.73      0.75       129\n",
      "         1.0       0.44      0.77      0.56        90\n",
      "         2.0       0.67      0.62      0.64       152\n",
      "         3.0       0.69      0.59      0.63       143\n",
      "         4.0       0.74      0.65      0.69       146\n",
      "         5.0       0.66      0.70      0.68       148\n",
      "         6.0       0.93      0.66      0.77       122\n",
      "         7.0       0.67      0.77      0.71       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.70      0.68      0.68      1050\n",
      "weighted avg       0.70      0.68      0.68      1050\n",
      "\n",
      "[80,     3] loss: 0.006\n",
      "acc:0.9238 \n",
      "err:0.0762 \n",
      "[Validation RESULT]\n",
      "acc:0.6905 \n",
      "err:0.3095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.74      0.78       129\n",
      "         1.0       0.52      0.71      0.60        90\n",
      "         2.0       0.65      0.62      0.64       152\n",
      "         3.0       0.79      0.59      0.68       143\n",
      "         4.0       0.62      0.76      0.68       146\n",
      "         5.0       0.64      0.65      0.64       148\n",
      "         6.0       0.84      0.72      0.78       122\n",
      "         7.0       0.74      0.76      0.75       120\n",
      "\n",
      "    accuracy                           0.69      1050\n",
      "   macro avg       0.70      0.69      0.69      1050\n",
      "weighted avg       0.71      0.69      0.69      1050\n",
      "\n",
      "[81,     3] loss: 0.006\n",
      "acc:0.9460 \n",
      "err:0.0540 \n",
      "[Validation RESULT]\n",
      "acc:0.6771 \n",
      "err:0.3229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.72      0.77       129\n",
      "         1.0       0.49      0.72      0.58        90\n",
      "         2.0       0.57      0.65      0.61       152\n",
      "         3.0       0.77      0.59      0.67       143\n",
      "         4.0       0.67      0.71      0.69       146\n",
      "         5.0       0.77      0.59      0.67       148\n",
      "         6.0       0.91      0.66      0.77       122\n",
      "         7.0       0.59      0.81      0.68       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.70      0.68      0.68      1050\n",
      "weighted avg       0.71      0.68      0.68      1050\n",
      "\n",
      "[82,     3] loss: 0.006\n",
      "acc:0.9175 \n",
      "err:0.0825 \n",
      "[Validation RESULT]\n",
      "acc:0.6686 \n",
      "err:0.3314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.71      0.70       129\n",
      "         1.0       0.47      0.67      0.55        90\n",
      "         2.0       0.63      0.62      0.63       152\n",
      "         3.0       0.52      0.69      0.59       143\n",
      "         4.0       0.73      0.65      0.69       146\n",
      "         5.0       0.69      0.68      0.68       148\n",
      "         6.0       0.94      0.67      0.78       122\n",
      "         7.0       0.93      0.67      0.78       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.70      0.67      0.68      1050\n",
      "weighted avg       0.70      0.67      0.68      1050\n",
      "\n",
      "[83,     3] loss: 0.005\n",
      "acc:0.9397 \n",
      "err:0.0603 \n",
      "[Validation RESULT]\n",
      "acc:0.6771 \n",
      "err:0.3229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.70      0.77       129\n",
      "         1.0       0.48      0.71      0.57        90\n",
      "         2.0       0.57      0.68      0.62       152\n",
      "         3.0       0.65      0.63      0.64       143\n",
      "         4.0       0.68      0.71      0.69       146\n",
      "         5.0       0.74      0.60      0.66       148\n",
      "         6.0       0.94      0.66      0.78       122\n",
      "         7.0       0.68      0.74      0.71       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.70      0.68      0.68      1050\n",
      "weighted avg       0.70      0.68      0.68      1050\n",
      "\n",
      "[84,     3] loss: 0.005\n",
      "acc:0.9429 \n",
      "err:0.0571 \n",
      "[Validation RESULT]\n",
      "acc:0.6819 \n",
      "err:0.3181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.71      0.78       129\n",
      "         1.0       0.52      0.54      0.53        90\n",
      "         2.0       0.60      0.64      0.62       152\n",
      "         3.0       0.52      0.75      0.61       143\n",
      "         4.0       0.64      0.75      0.69       146\n",
      "         5.0       0.79      0.60      0.68       148\n",
      "         6.0       0.89      0.71      0.79       122\n",
      "         7.0       0.86      0.71      0.78       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.71      0.68      0.69      1050\n",
      "weighted avg       0.71      0.68      0.69      1050\n",
      "\n",
      "[85,     3] loss: 0.005\n",
      "acc:0.9365 \n",
      "err:0.0635 \n",
      "[Validation RESULT]\n",
      "acc:0.6838 \n",
      "err:0.3162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.75      0.72       129\n",
      "         1.0       0.51      0.63      0.56        90\n",
      "         2.0       0.71      0.59      0.65       152\n",
      "         3.0       0.65      0.65      0.65       143\n",
      "         4.0       0.69      0.69      0.69       146\n",
      "         5.0       0.65      0.70      0.67       148\n",
      "         6.0       0.92      0.69      0.79       122\n",
      "         7.0       0.71      0.77      0.74       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.69      0.68      0.68      1050\n",
      "weighted avg       0.70      0.68      0.69      1050\n",
      "\n",
      "[86,     3] loss: 0.005\n",
      "acc:0.9365 \n",
      "err:0.0635 \n",
      "[Validation RESULT]\n",
      "acc:0.6829 \n",
      "err:0.3171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.67      0.79       129\n",
      "         1.0       0.52      0.62      0.57        90\n",
      "         2.0       0.64      0.62      0.63       152\n",
      "         3.0       0.60      0.70      0.65       143\n",
      "         4.0       0.61      0.79      0.69       146\n",
      "         5.0       0.75      0.59      0.66       148\n",
      "         6.0       0.89      0.70      0.79       122\n",
      "         7.0       0.68      0.75      0.71       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.71      0.68      0.69      1050\n",
      "weighted avg       0.71      0.68      0.69      1050\n",
      "\n",
      "[87,     3] loss: 0.005\n",
      "acc:0.9460 \n",
      "err:0.0540 \n",
      "[Validation RESULT]\n",
      "acc:0.6790 \n",
      "err:0.3210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.73      0.73       129\n",
      "         1.0       0.46      0.66      0.54        90\n",
      "         2.0       0.66      0.61      0.63       152\n",
      "         3.0       0.69      0.64      0.66       143\n",
      "         4.0       0.69      0.68      0.68       146\n",
      "         5.0       0.70      0.66      0.68       148\n",
      "         6.0       0.68      0.79      0.73       122\n",
      "         7.0       0.90      0.69      0.78       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.69      0.68      0.68      1050\n",
      "weighted avg       0.69      0.68      0.68      1050\n",
      "\n",
      "[88,     3] loss: 0.005\n",
      "acc:0.9460 \n",
      "err:0.0540 \n",
      "[Validation RESULT]\n",
      "acc:0.6571 \n",
      "err:0.3429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.73      0.71       129\n",
      "         1.0       0.63      0.32      0.43        90\n",
      "         2.0       0.77      0.58      0.66       152\n",
      "         3.0       0.49      0.71      0.58       143\n",
      "         4.0       0.68      0.71      0.70       146\n",
      "         5.0       0.55      0.74      0.63       148\n",
      "         6.0       0.94      0.64      0.76       122\n",
      "         7.0       0.79      0.72      0.75       120\n",
      "\n",
      "    accuracy                           0.66      1050\n",
      "   macro avg       0.69      0.64      0.65      1050\n",
      "weighted avg       0.69      0.66      0.66      1050\n",
      "\n",
      "[89,     3] loss: 0.005\n",
      "acc:0.9429 \n",
      "err:0.0571 \n",
      "[Validation RESULT]\n",
      "acc:0.6819 \n",
      "err:0.3181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.69      0.80       129\n",
      "         1.0       0.49      0.59      0.54        90\n",
      "         2.0       0.61      0.66      0.63       152\n",
      "         3.0       0.51      0.73      0.60       143\n",
      "         4.0       0.65      0.74      0.69       146\n",
      "         5.0       0.78      0.61      0.69       148\n",
      "         6.0       0.81      0.75      0.77       122\n",
      "         7.0       0.96      0.66      0.78       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.72      0.68      0.69      1050\n",
      "weighted avg       0.72      0.68      0.69      1050\n",
      "\n",
      "[90,     3] loss: 0.005\n",
      "acc:0.9429 \n",
      "err:0.0571 \n",
      "[Validation RESULT]\n",
      "acc:0.6810 \n",
      "err:0.3190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.75      0.70       129\n",
      "         1.0       0.51      0.67      0.58        90\n",
      "         2.0       0.76      0.57      0.65       152\n",
      "         3.0       0.75      0.62      0.67       143\n",
      "         4.0       0.67      0.69      0.68       146\n",
      "         5.0       0.64      0.68      0.66       148\n",
      "         6.0       0.91      0.70      0.80       122\n",
      "         7.0       0.64      0.79      0.71       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.69      0.68      0.68      1050\n",
      "weighted avg       0.70      0.68      0.68      1050\n",
      "\n",
      "[91,     3] loss: 0.005\n",
      "acc:0.9302 \n",
      "err:0.0698 \n",
      "[Validation RESULT]\n",
      "acc:0.6829 \n",
      "err:0.3171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.70      0.81       129\n",
      "         1.0       0.53      0.61      0.57        90\n",
      "         2.0       0.53      0.72      0.61       152\n",
      "         3.0       0.60      0.66      0.63       143\n",
      "         4.0       0.71      0.69      0.70       146\n",
      "         5.0       0.68      0.65      0.66       148\n",
      "         6.0       0.85      0.70      0.77       122\n",
      "         7.0       0.80      0.72      0.75       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.71      0.68      0.69      1050\n",
      "weighted avg       0.71      0.68      0.69      1050\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92,     3] loss: 0.005\n",
      "acc:0.9460 \n",
      "err:0.0540 \n",
      "[Validation RESULT]\n",
      "acc:0.6905 \n",
      "err:0.3095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.72      0.80       129\n",
      "         1.0       0.50      0.76      0.60        90\n",
      "         2.0       0.62      0.61      0.62       152\n",
      "         3.0       0.79      0.59      0.67       143\n",
      "         4.0       0.62      0.75      0.68       146\n",
      "         5.0       0.68      0.65      0.66       148\n",
      "         6.0       0.86      0.73      0.79       122\n",
      "         7.0       0.70      0.77      0.73       120\n",
      "\n",
      "    accuracy                           0.69      1050\n",
      "   macro avg       0.71      0.70      0.69      1050\n",
      "weighted avg       0.71      0.69      0.69      1050\n",
      "\n",
      "[93,     3] loss: 0.005\n",
      "acc:0.9683 \n",
      "err:0.0317 \n",
      "[Validation RESULT]\n",
      "acc:0.6771 \n",
      "err:0.3229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.70      0.74       129\n",
      "         1.0       0.51      0.63      0.56        90\n",
      "         2.0       0.71      0.57      0.64       152\n",
      "         3.0       0.50      0.73      0.59       143\n",
      "         4.0       0.62      0.74      0.68       146\n",
      "         5.0       0.69      0.67      0.68       148\n",
      "         6.0       0.93      0.70      0.80       122\n",
      "         7.0       0.96      0.67      0.79       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.72      0.68      0.68      1050\n",
      "weighted avg       0.71      0.68      0.68      1050\n",
      "\n",
      "[94,     3] loss: 0.004\n",
      "acc:0.9556 \n",
      "err:0.0444 \n",
      "[Validation RESULT]\n",
      "acc:0.6829 \n",
      "err:0.3171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.71      0.80       129\n",
      "         1.0       0.49      0.68      0.57        90\n",
      "         2.0       0.58      0.68      0.63       152\n",
      "         3.0       0.63      0.64      0.63       143\n",
      "         4.0       0.68      0.71      0.70       146\n",
      "         5.0       0.73      0.65      0.69       148\n",
      "         6.0       0.88      0.68      0.77       122\n",
      "         7.0       0.70      0.73      0.72       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.70      0.68      0.69      1050\n",
      "weighted avg       0.70      0.68      0.69      1050\n",
      "\n",
      "[95,     3] loss: 0.004\n",
      "acc:0.9683 \n",
      "err:0.0317 \n",
      "[Validation RESULT]\n",
      "acc:0.6810 \n",
      "err:0.3190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.71      0.74       129\n",
      "         1.0       0.50      0.59      0.54        90\n",
      "         2.0       0.58      0.71      0.64       152\n",
      "         3.0       0.54      0.73      0.62       143\n",
      "         4.0       0.69      0.71      0.70       146\n",
      "         5.0       0.77      0.59      0.67       148\n",
      "         6.0       0.91      0.70      0.79       122\n",
      "         7.0       0.89      0.70      0.79       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.71      0.68      0.69      1050\n",
      "weighted avg       0.71      0.68      0.69      1050\n",
      "\n",
      "[96,     3] loss: 0.004\n",
      "acc:0.9778 \n",
      "err:0.0222 \n",
      "[Validation RESULT]\n",
      "acc:0.6905 \n",
      "err:0.3095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.71      0.76       129\n",
      "         1.0       0.52      0.69      0.59        90\n",
      "         2.0       0.67      0.64      0.65       152\n",
      "         3.0       0.62      0.67      0.64       143\n",
      "         4.0       0.70      0.68      0.69       146\n",
      "         5.0       0.67      0.69      0.68       148\n",
      "         6.0       0.86      0.71      0.78       122\n",
      "         7.0       0.73      0.75      0.74       120\n",
      "\n",
      "    accuracy                           0.69      1050\n",
      "   macro avg       0.70      0.69      0.69      1050\n",
      "weighted avg       0.70      0.69      0.69      1050\n",
      "\n",
      "[97,     3] loss: 0.004\n",
      "acc:0.9714 \n",
      "err:0.0286 \n",
      "[Validation RESULT]\n",
      "acc:0.6829 \n",
      "err:0.3171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.71      0.74       129\n",
      "         1.0       0.50      0.68      0.58        90\n",
      "         2.0       0.68      0.61      0.64       152\n",
      "         3.0       0.58      0.66      0.62       143\n",
      "         4.0       0.68      0.71      0.70       146\n",
      "         5.0       0.63      0.72      0.67       148\n",
      "         6.0       0.94      0.68      0.79       122\n",
      "         7.0       0.82      0.70      0.75       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.70      0.68      0.69      1050\n",
      "weighted avg       0.70      0.68      0.69      1050\n",
      "\n",
      "[98,     3] loss: 0.003\n",
      "acc:0.9746 \n",
      "err:0.0254 \n",
      "[Validation RESULT]\n",
      "acc:0.6924 \n",
      "err:0.3076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.72      0.76       129\n",
      "         1.0       0.46      0.74      0.57        90\n",
      "         2.0       0.66      0.68      0.67       152\n",
      "         3.0       0.74      0.60      0.66       143\n",
      "         4.0       0.71      0.71      0.71       146\n",
      "         5.0       0.72      0.64      0.67       148\n",
      "         6.0       0.85      0.73      0.78       122\n",
      "         7.0       0.68      0.77      0.72       120\n",
      "\n",
      "    accuracy                           0.69      1050\n",
      "   macro avg       0.70      0.70      0.69      1050\n",
      "weighted avg       0.71      0.69      0.70      1050\n",
      "\n",
      "[99,     3] loss: 0.004\n",
      "acc:0.9746 \n",
      "err:0.0254 \n",
      "[Validation RESULT]\n",
      "acc:0.6667 \n",
      "err:0.3333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.69      0.78       129\n",
      "         1.0       0.64      0.46      0.53        90\n",
      "         2.0       0.61      0.65      0.63       152\n",
      "         3.0       0.40      0.85      0.55       143\n",
      "         4.0       0.72      0.67      0.69       146\n",
      "         5.0       0.75      0.61      0.68       148\n",
      "         6.0       0.96      0.66      0.79       122\n",
      "         7.0       0.94      0.67      0.78       120\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.74      0.66      0.68      1050\n",
      "weighted avg       0.74      0.67      0.68      1050\n",
      "\n",
      "[100,     3] loss: 0.004\n",
      "acc:0.9492 \n",
      "err:0.0508 \n",
      "[Validation RESULT]\n",
      "acc:0.6781 \n",
      "err:0.3219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.71      0.75       129\n",
      "         1.0       0.56      0.49      0.52        90\n",
      "         2.0       0.68      0.63      0.65       152\n",
      "         3.0       0.52      0.72      0.60       143\n",
      "         4.0       0.65      0.73      0.69       146\n",
      "         5.0       0.69      0.64      0.67       148\n",
      "         6.0       0.83      0.75      0.78       122\n",
      "         7.0       0.79      0.71      0.75       120\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.69      0.67      0.68      1050\n",
      "weighted avg       0.69      0.68      0.68      1050\n",
      "\n",
      "2 {'Accuracy': [0.13, 0.15, 0.1, 0.13, 0.16, 0.17, 0.25, 0.21, 0.31, 0.31, 0.27, 0.38, 0.49, 0.46, 0.43, 0.5, 0.48, 0.54, 0.55, 0.55, 0.59, 0.49, 0.58, 0.57, 0.49, 0.56, 0.58, 0.7, 0.6, 0.67, 0.72, 0.71, 0.75, 0.79, 0.73, 0.81, 0.8, 0.82, 0.82, 0.83, 0.82, 0.76, 0.75, 0.78, 0.72, 0.8, 0.82, 0.83, 0.83, 0.86, 0.87, 0.86, 0.88, 0.88, 0.88, 0.87, 0.87, 0.78, 0.79, 0.73, 0.71, 0.69, 0.71, 0.8, 0.75, 0.66, 0.64, 0.64, 0.77, 0.79, 0.84, 0.86, 0.81, 0.89, 0.91, 0.93, 0.9, 0.93, 0.91, 0.92, 0.95, 0.92, 0.94, 0.94, 0.94, 0.94, 0.95, 0.95, 0.94, 0.94, 0.93, 0.95, 0.97, 0.96, 0.97, 0.98, 0.97, 0.97, 0.97, 0.95], 'F1_Score': [0.07, 0.08, 0.04, 0.08, 0.1, 0.07, 0.21, 0.16, 0.29, 0.23, 0.26, 0.3, 0.5, 0.43, 0.4, 0.47, 0.47, 0.49, 0.55, 0.55, 0.56, 0.49, 0.55, 0.58, 0.46, 0.56, 0.57, 0.7, 0.59, 0.67, 0.71, 0.72, 0.74, 0.79, 0.74, 0.81, 0.8, 0.82, 0.83, 0.83, 0.82, 0.76, 0.72, 0.78, 0.72, 0.8, 0.82, 0.82, 0.82, 0.86, 0.87, 0.85, 0.87, 0.88, 0.88, 0.87, 0.87, 0.76, 0.79, 0.73, 0.69, 0.7, 0.71, 0.8, 0.76, 0.67, 0.63, 0.65, 0.77, 0.79, 0.84, 0.86, 0.8, 0.88, 0.91, 0.93, 0.9, 0.93, 0.91, 0.92, 0.94, 0.91, 0.94, 0.94, 0.94, 0.93, 0.94, 0.94, 0.94, 0.94, 0.93, 0.94, 0.97, 0.95, 0.97, 0.98, 0.97, 0.97, 0.97, 0.95]}\n",
      "2 {'Accuracy': [0.12, 0.09, 0.14, 0.14, 0.18, 0.18, 0.16, 0.23, 0.31, 0.23, 0.35, 0.4, 0.49, 0.38, 0.45, 0.44, 0.4, 0.47, 0.46, 0.51, 0.43, 0.49, 0.48, 0.49, 0.45, 0.52, 0.58, 0.58, 0.53, 0.6, 0.61, 0.58, 0.64, 0.6, 0.61, 0.66, 0.66, 0.66, 0.66, 0.66, 0.65, 0.58, 0.61, 0.64, 0.66, 0.67, 0.63, 0.66, 0.64, 0.66, 0.64, 0.66, 0.67, 0.66, 0.67, 0.63, 0.64, 0.62, 0.59, 0.57, 0.59, 0.56, 0.64, 0.57, 0.53, 0.53, 0.52, 0.62, 0.65, 0.66, 0.65, 0.61, 0.66, 0.68, 0.67, 0.67, 0.68, 0.68, 0.68, 0.69, 0.68, 0.67, 0.68, 0.68, 0.68, 0.68, 0.68, 0.66, 0.68, 0.68, 0.68, 0.69, 0.68, 0.68, 0.68, 0.69, 0.68, 0.69, 0.67, 0.68], 'F1_Score': [0.03, 0.02, 0.03, 0.03, 0.06, 0.15, 0.08, 0.11, 0.25, 0.18, 0.25, 0.4, 0.43, 0.32, 0.44, 0.39, 0.34, 0.48, 0.41, 0.48, 0.42, 0.43, 0.49, 0.45, 0.4, 0.53, 0.58, 0.54, 0.52, 0.58, 0.65, 0.55, 0.64, 0.62, 0.62, 0.67, 0.66, 0.67, 0.66, 0.67, 0.65, 0.56, 0.63, 0.66, 0.67, 0.67, 0.6, 0.66, 0.65, 0.66, 0.63, 0.66, 0.67, 0.66, 0.67, 0.65, 0.62, 0.62, 0.59, 0.55, 0.58, 0.54, 0.65, 0.6, 0.55, 0.51, 0.54, 0.62, 0.65, 0.67, 0.65, 0.61, 0.65, 0.69, 0.67, 0.67, 0.68, 0.68, 0.68, 0.69, 0.68, 0.68, 0.68, 0.69, 0.68, 0.69, 0.68, 0.65, 0.69, 0.68, 0.69, 0.69, 0.68, 0.69, 0.69, 0.69, 0.69, 0.69, 0.68, 0.68]}\n",
      "Finished Training 100\n"
     ]
    }
   ],
   "source": [
    "!python MLP_baseline.py --mode train --data emotiontoronto --epoch 100 --lr 4.e-3 --batch_size 128 --train_dataset_percentage 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    classification\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    pairwise_fast\n",
      "    ranking\n",
      "    regression\n",
      "    scorer\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jaccard_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, average_method='warn')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : string, optional (default: 'warn')\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "            If 'warn', 'max' will be used. The default will change to\n",
      "            'arithmetic' in version 0.22.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        mutual_info_score: Mutual Information (not adjusted for chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ari : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not always pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "    \n",
      "    auc(x, y, reorder='deprecated')\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array, shape = [n]\n",
      "            x coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : array, shape = [n]\n",
      "            y coordinates.\n",
      "        reorder : boolean, optional (default='deprecated')\n",
      "            Whether to sort x before computing. If False, assume that x must be\n",
      "            either monotonic increasing or monotonic decreasing. If True, y is\n",
      "            used to break ties when sorting x. Make sure that y has a monotonic\n",
      "            relation to x when setting reorder to True.\n",
      "        \n",
      "            .. deprecated:: 0.20\n",
      "               Parameter ``reorder`` has been deprecated in version 0.20 and will\n",
      "               be removed in 0.22. It's introduced for roc_auc_score (not for\n",
      "               general use) and is no longer used there. What's more, the result\n",
      "               from auc will be significantly influenced if x is sorted\n",
      "               unexpectedly due to slight floating point error (See issue #9786).\n",
      "               Future (and default) behavior is equivalent to ``reorder=False``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        average_precision_score : Compute average precision from prediction scores\n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "    \n",
      "    average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str (default=1)\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        \n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)  # doctest: +ELLIPSIS\n",
      "        0.83...\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, and perfect performance scores 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        recall_score, roc_auc_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score.\n",
      "        The smaller the Brier score, the better, hence the naming with \"loss\".\n",
      "        Across all items in a set N predictions, the Brier score measures the\n",
      "        mean squared difference between (1) the predicted probability assigned\n",
      "        to the possible outcomes for item i, and (2) the actual outcome.\n",
      "        Therefore, the lower the Brier score is for a set of predictions, the\n",
      "        better the predictions are calibrated. Note that the Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). The Brier loss is composed of refinement loss and\n",
      "        calibration loss.\n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter pos_label, which defaults to 1.\n",
      "        Read more in the :ref:`User Guide <calibration>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array, shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class.\n",
      "            Defaults to the greater label unless y_true is all 0 or all -1\n",
      "            in which case pos_label defaults to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob,                          pos_label=\"ham\")  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score.\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (``n_samples``, ``n_features``)\n",
      "            List of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like, shape (``n_samples``,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    calinski_harabaz_score(X, labels)\n",
      "        DEPRECATED: Function 'calinski_harabaz_score' has been renamed to 'calinski_harabasz_score' and will be removed in version 0.23.\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : string, callable or None, optional, default: None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        allow_none : boolean, optional, default: False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)\n",
      "        Build a text report showing the main classification metrics\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels]\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of strings\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool (default = False)\n",
      "            If True, return output as dict\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string / dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), sample average (only for multilabel classification) and\n",
      "            micro average (averaging the total true positives, false negatives and\n",
      "            false positives) it is only shown for multi-label or multi-class\n",
      "            with a subset of classes because it is accuracy otherwise.\n",
      "            See also:func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array, shape = [n_samples]\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array, shape = [n_samples]\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : str, optional\n",
      "            List of weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596.\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa.\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` but\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If none is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : array, shape = [n_classes, n_classes]\n",
      "            Confusion matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : string or function, optional, default: \"jaccard\"\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, sample_weight=None)\n",
      "        Coverage error measure\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Computes the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (``n_samples``, ``n_features``)\n",
      "            List of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like, shape (``n_samples``,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation, and\n",
      "        the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)\n",
      "        \n",
      "        Y_norm_squared : array-like, shape (n_samples_2, ), optional\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : boolean, optional\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like, shape = [n_samples_1], optional\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve better accuracy, `X_norm_squared`and `Y_norm_squared` may be\n",
      "        unused if they are passed as ``float32``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : array, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        paired_distances : distances betweens pairs of elements of X and Y.\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F-beta score\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Weight of precision in harmonic mean.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str | callable\n",
      "            scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels], optional (default='deprecated')\n",
      "            Integer array of labels. If not provided, labels will be inferred\n",
      "            from y_true and y_pred.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "            .. deprecated:: 0.21\n",
      "               This parameter ``labels`` is deprecated in version 0.21 and will\n",
      "               be removed in version 0.23. Hamming loss uses ``y_true.shape[1]``\n",
      "               for the number of labels when y_true is binary label indicators,\n",
      "               so it is unnecessary for the user to specify.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized)\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array, optional, default None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero.\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision  # doctest: +ELLIPSIS\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, f_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    jaccard_similarity_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        .. deprecated:: 0.21\n",
      "            This is deprecated to be removed in 0.23, since its handling of\n",
      "            binary and multiclass inputs was broken. `jaccard_score` has an API\n",
      "            that is consistent with precision_score, f_score, etc.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the sum of the Jaccard similarity coefficient\n",
      "            over the sample set. Otherwise, return the average of Jaccard\n",
      "            similarity coefficient.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the average Jaccard similarity\n",
      "            coefficient, else it returns the sum of the Jaccard similarity\n",
      "            coefficient over the sample set.\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equivalent\n",
      "        to the ``accuracy_score``. It differs in the multilabel classification\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, sample_weight=None)\n",
      "        Compute ranking-based average precision\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, sample_weight=None)\n",
      "        Compute Ranking loss measure\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of the true labels given a probabilistic classifier's\n",
      "        predictions. The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label yt in {0,1} and\n",
      "        estimated probability yp that yt = 1, the log loss is\n",
      "        \n",
      "            -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, optional (default=None)\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "    \n",
      "    make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in GridSearchCV\n",
      "        and cross_val_score. It takes a score function, such as ``accuracy_score``,\n",
      "        ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable,\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : boolean, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : boolean, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "        needs_threshold : boolean, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC)\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], default None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.85...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']             or array-like of shape = (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.060...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred)\n",
      "        Median absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Estimated targets as returned by a classifier\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples,), optional\n",
      "            Sample weights\n",
      "        \n",
      "        labels : array-like\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data)\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : array, shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while confusion_matrix calculates\n",
      "        one confusion matrix for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels of\n",
      "        the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        contingency : {None, array, sparse matrix},                   shape = [n_classes_true, n_classes_pred]\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted against chance Mutual Information\n",
      "        normalized_mutual_info_score: Normalized Mutual Information\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, average_method='warn')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : string, optional (default: 'warn')\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "            If 'warn', 'geometric' will be used. The default will change to\n",
      "            'arithmetic' in version 0.22.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        v_measure_score: V-Measure (NMI with arithmetic mean option.)\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n",
      "            against chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', n_jobs=None, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix inputs.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        pairwise_distances_chunked : performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding\n",
      "                           elements of two arrays\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, axis=1, metric='euclidean', batch_size=None, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        Y : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            .. deprecated:: 0.20\n",
      "                Deprecated for removal in 0.22.\n",
      "                Use sklearn.set_config(working_memory=...) instead.\n",
      "        \n",
      "        metric_kwargs : dict\n",
      "            keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, axis=1, metric='euclidean', batch_size=None, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples1, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples2, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable, default 'euclidean'\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            .. deprecated:: 0.20\n",
      "                Deprecated for removal in 0.22.\n",
      "                Use sklearn.set_config(working_memory=...) instead.\n",
      "        \n",
      "        metric_kwargs : dict, optional\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : numpy.ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,\n",
      "            [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, optional\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return an array, a list, or a sparse matrix of length\n",
      "            ``D_chunk.shape[0]``, or a tuple of such objects.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : array or sparse matrix\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk  # doctest: +ELLIPSIS\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist  # doctest: +ELLIPSIS\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are::\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "             'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features]\n",
      "            A second feature array only if X has shape [n_samples_a, n_features].\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        filter_params : boolean\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int or None, optional (default=None)\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold.  This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : array, shape = [n_samples]\n",
      "            Estimated probabilities or decision function.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : array, shape = [n_thresholds + 1]\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : array, shape = [n_thresholds + 1]\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))]\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision from prediction scores\n",
      "        \n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision  # doctest: +ELLIPSIS\n",
      "        array([0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None)\n",
      "        Compute precision, recall, F-measure and support for each class\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, 1.0 by default\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None (default), 'binary', 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : int (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined;\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, the metric will be set to 0, as will f-score, and\n",
      "        ``UndefinedMetricWarning`` will be raised.\n",
      "    \n",
      "    precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the precision\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``.\n",
      "    \n",
      "    r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        R^2 (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a R^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average', 'variance_weighted'] or None or array-like of shape (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The R^2 score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, R^2 score may be negative (it need not actually\n",
      "        be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted') # doctest: +ELLIPSIS\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the recall\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, balanced_accuracy_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, average='macro', sample_weight=None, max_fpr=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
      "        from prediction scores.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task in label indicator format.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers). For binary\n",
      "            y_true, y_score is supposed to be the score of the class with greater\n",
      "            label.\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, optional\n",
      "            If not ``None``, the standardized partial AUC [3]_ over the range\n",
      "            [0, max_fpr] is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        .. [3] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve\n",
      "        \n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> roc_auc_score(y_true, y_scores)\n",
      "        0.75\n",
      "    \n",
      "    roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC)\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        y_true : array, shape = [n_samples]\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array, shape = [n_samples]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : boolean, optional (default=True)\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : array, shape = [>2]\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        tpr : array, shape = [>2]\n",
      "            Increasing true positive rates such that element i is the true\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds]\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "                 label values for each sample\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`. If X is\n",
      "            the distance array itself, use \"precomputed\" as the metric.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array, shape = [n_samples]\n",
      "            Silhouette Coefficient for each samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "             Predicted labels for each sample.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If X is the distance\n",
      "            array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int or None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            The generator used to randomly select a subset of samples.  If int,\n",
      "            random_state is the seed used by the random number generator; If\n",
      "            RandomState instance, random_state is the random number generator; If\n",
      "            None, the random number generator is the RandomState instance used by\n",
      "            `np.random`. Used when ``sample_size is not None``.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        normalized_mutual_info_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
